{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "import json\n",
    "import pickle\n",
    "import torch, random, copy\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "import plotly.express as pe\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.engine import train_one_epoch, evaluate\n",
    "from utils import utils\n",
    "\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from utils import transforms as T\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ADE20k_classes.pkl', 'rb') as f:\n",
    "    classes = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img_pt, trues, preds=None):\n",
    "    img = F.to_pil_image(img_pt)\n",
    "    size = img.size\n",
    "    fig = pe.imshow(img)\n",
    "    for box, label_list in zip(trues[\"boxes\"],trues[\"labels\"]):\n",
    "        label_text = classes[label_list]\n",
    "        fig.add_shape(type='rect',\n",
    "                      x0=box[0], x1=box[2], \n",
    "                      y0=box[1], y1=box[3],\n",
    "                      xref='x', yref='y',\n",
    "                      line=dict(color='green', width=2))\n",
    "        fig.add_annotation(x=box[0], y=box[1], text=str(classes[label_list.item()]),\n",
    "                           xref=\"x\", yref=\"y\", showarrow=False,\n",
    "                           font_size=10, font_color='green',\n",
    "                           bgcolor=\"white\")\n",
    "    if preds != None:\n",
    "        for box, label_list, score in zip(preds[\"boxes\"].cpu().tolist(), preds[\"labels\"].cpu().tolist(), preds[\"scores\"].cpu().tolist(),):\n",
    "            label_text = classes[label_list]\n",
    "            fig.add_shape(type='rect',\n",
    "                          x0=box[0], x1=box[2], \n",
    "                          y0=box[1], y1=box[3],\n",
    "                          xref='x', yref='y',\n",
    "                          line=dict(color='red', width=2, dash='dot'))\n",
    "            fig.add_annotation(x=box[2], y=box[1], text=f\"{classes[label_list]} {score*100:.1f}%\",\n",
    "                               xref=\"x\", yref=\"y\", showarrow=False,\n",
    "                               font_size=10, font_color='red',\n",
    "                               bgcolor=\"white\")    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADEDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root,  filename, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.image_list = self.read_file(os.path.join(root, filename))\n",
    "        self.len = len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        image_name= self.image_list[idx]\n",
    "        img_path = os.path.join(self.root, \"imgs\", f\"ADE_val_{image_name}.jpg\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        json_path = os.path.join(self.root, \"jsons\", f\"ADE_val_{image_name}.json\")\n",
    "        mask_path = os.path.join(self.root, \"instance_mask_backup\")\n",
    "        \n",
    "        with open(json_path) as f:\n",
    "            img_data = json.load(f)\n",
    "\n",
    "        label = []\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        for obj in img_data['annotation']['object']:\n",
    "            id = obj['id']\n",
    "            obj_name = obj['name'].split(\",\")\n",
    "            for single_name in obj_name:    \n",
    "                single_name = single_name.strip() \n",
    "                if  single_name in classes.keys():\n",
    "                    xmin = min(obj['polygon']['x'])\n",
    "                    xmax = max(obj['polygon']['x'])\n",
    "                    ymin = min(obj['polygon']['y'])\n",
    "                    ymax = max(obj['polygon']['y'])\n",
    "                    if xmin == xmax or ymin == ymax:\n",
    "                        break\n",
    "                        \n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    \n",
    "                    label.append(classes[single_name])\n",
    "\n",
    "                    instance_path = os.path.join(mask_path, obj[\"instance_mask\"])\n",
    "                    mask = np.array(Image.open(instance_path))\n",
    "                    masks.append(mask)\n",
    "                    break\n",
    "\n",
    "        boxes = np.array(boxes)\n",
    "        # convert everything into a torch.Tensor\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(np.array(label), dtype=torch.int64) - 1\n",
    "        target[\"masks\"] = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] =torch.as_tensor(area, dtype=torch.uint8)\n",
    "        target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def read_file(self,filename):\n",
    "        image_list = []\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # rstrip：用来去除结尾字符、空白符(包括\\n、\\r、\\t、' '，即：换行、回车、制表符、空格)\n",
    "                img = line.rstrip().split(' ')[0]\n",
    "                image_list.append(img)\n",
    "        return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = \"../ADE20K\"\n",
    "# image_name = \"00000001\"\n",
    "# img_path = os.path.join(root, \"imgs\", f\"ADE_val_{image_name}.jpg\")\n",
    "# #mask_path = os.path.join(root, \"masks\", f\"ADE_val_{image_name}_seg.png\")\n",
    "# json_path = os.path.join(root, \"jsons\", f\"ADE_val_{image_name}.json\")\n",
    "# mask_path = os.path.join(root, \"instance_mask_backup\")#, f\"ADE_val_{image_name}\")\n",
    "\n",
    "# img = Image.open(img_path).convert(\"RGB\")\n",
    "# # mask = Image.open(mask_path).convert(\"RGB\")\n",
    "# # # convert the PIL Image into a numpy array\n",
    "# # mask = ImageOps.grayscale(mask)\n",
    "# # obj_ids = np.unique(mask)[1:]\n",
    "# # # of binary masks\n",
    "\n",
    "# #masks = mask == obj_ids[:, None, None]\n",
    "# with open(json_path) as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# label = []\n",
    "# masks = []\n",
    "# for obj in data['annotation']['object']:\n",
    "#     id = obj['id']\n",
    "#     obj_name = obj['name'].split(\",\")\n",
    "#     for single_name in obj_name:    \n",
    "#         single_name = single_name.strip() \n",
    "#         if  single_name in classes.keys():\n",
    "#             label.append(classes[single_name])\n",
    "#             # print(obj_name, single_name, classes[single_name])\n",
    "#             instance_path = os.path.join(mask_path, obj[\"instance_mask\"])#f\"instance_{id:03d}_ADE_val_{image_name}.png\")\n",
    "#             #print(id, instance_path)\n",
    "#             mask = np.array(Image.open(instance_path))\n",
    "#             masks.append(mask)\n",
    "\n",
    "#             break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../ADE20K\"\n",
    "train_data = ADEDataset(root,\"train.txt\", get_transform(train=True))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                                train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                                collate_fn=utils.collate_fn)\n",
    "val_data = ADEDataset(root,\"val.txt\", get_transform(train=True))\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "                                val_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                                collate_fn=utils.collate_fn)\n",
    "test_data = ADEDataset(root,\"test.txt\", get_transform(train=True))\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                                test_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                                collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train dataset :{len(train_data)} images\")\n",
    "print(f\"val dataset :{len(val_data)} images\")\n",
    "print(f\"test dataset :{len(test_data)} images\")\n",
    "# print(f\"the meta data in image:{voctrainval_ds[0][1].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "weight_decay=1e-5\n",
    "num_classes = 150\n",
    "model = get_model_instance_segmentation(num_classes).to(device)\n",
    "#model = models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(score_thresh=0.5, weights_backbone=True, num_classes=num_classes).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr = lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    metric_logger = train_one_epoch(model, optimizer, train_dataloader, device, epoch=epoch, run_dataset= \"ade\", print_freq=10, scaler=None)\n",
    "    lr_scheduler.step()\n",
    "        # evaluate on the test dataset3\n",
    "    evaluate(model, val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
