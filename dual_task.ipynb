{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from utils.engine import train_one_epoch, evaluate\n",
    "from utils import utils\n",
    "\n",
    "import torch\n",
    "from utils import transforms as T\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "from ADEdataset import ADEDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "compose = transforms.Compose([transforms.PILToTensor(),\n",
    "                                transforms.ConvertImageDtype(torch.float),\n",
    "                                transforms.RandomHorizontalFlip(0.5),])\n",
    "                                #transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "def voc_transform_ing(img):\n",
    "    \n",
    "    target_anno = img[\"annotation\"]\n",
    "    w, h, d = list(map(int, target_anno['size'].values()))\n",
    "\n",
    "    target = {\"boxes\":[], \"labels\":[],'area':[]}\n",
    "    for obj in target_anno['object']:\n",
    "        box = list(map(int,obj['bndbox'].values()))\n",
    "        target[\"boxes\"].append(box)\n",
    "        target['labels'].append(classes.index(obj[\"name\"]))\n",
    "        \n",
    "        area = (box[3] - box[1]) * (box[2] - box[0])\n",
    "        target[\"area\"].append(area)    \n",
    "\n",
    "    iscrowd = torch.zeros((len(target[\"boxes\"]),), dtype=torch.int64)\n",
    "\n",
    "    target[\"image_id\"] = torch.Tensor([int(target_anno['filename'][:-4])])\n",
    "    target[\"boxes\"] = torch.FloatTensor(target[\"boxes\"])\n",
    "    target[\"labels\"] = torch.LongTensor(target[\"labels\"])\n",
    "    target[\"area\"] = torch.FloatTensor(target[\"area\"])\n",
    "    target[\"iscrowd\"] = iscrowd\n",
    "    target[\"masks\"] = torch.zeros((len(target[\"boxes\"]),w, h), dtype=torch.float32)\n",
    "\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "           'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "           'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "\n",
    "path_dir_trainval = \"../hw4_dataset/VOCTrainVal\"\n",
    "path_dir_test = \"../hw4_dataset/VOCtest\"\n",
    "\n",
    "voctrainval_ds = datasets.VOCDetection(root=path_dir_trainval, year=\"2007\", image_set=\"trainval\", transform=compose, target_transform=voc_transform_ing)\n",
    "voc_trainval = DataLoader(voctrainval_ds, batch_size=8, collate_fn = utils.collate_fn, shuffle=True)\n",
    "\n",
    "voctest_ds = datasets.VOCDetection(root=path_dir_test, year=\"2007\", image_set=\"test\", transform=compose,target_transform=voc_transform_ing)\n",
    "voc_test = DataLoader(voctest_ds, batch_size=8, collate_fn = utils.collate_fn, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"trainval dataset :{len(voctrainval_ds)} images\")\n",
    "print(f\"test dataset :{len(voctest_ds)} images\")\n",
    "print(f\"the meta data in image:{voctrainval_ds[0][1].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../ADE20K\"\n",
    "train_data = ADEDataset(root,\"train.txt\", get_transform(train=True))\n",
    "ade_train = torch.utils.data.DataLoader(\n",
    "                                train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                                collate_fn=utils.collate_fn)\n",
    "val_data = ADEDataset(root,\"val.txt\",  get_transform(train=True))\n",
    "ade_val = torch.utils.data.DataLoader(\n",
    "                                val_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                                collate_fn=utils.collate_fn)\n",
    "test_data = ADEDataset(root,\"test.txt\",  get_transform(train=True))\n",
    "ade_test = torch.utils.data.DataLoader(\n",
    "                                test_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "                                collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train dataset :{len(train_data)} images\")\n",
    "print(f\"val dataset :{len(val_data)} images\")\n",
    "print(f\"test dataset :{len(test_data)} images\")\n",
    "# print(f\"the meta data in image:{voctrainval_ds[0][1].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "weight_decay=1e-5\n",
    "num_classes = 150\n",
    "model = get_model_instance_segmentation(num_classes).to(device)\n",
    "#model = models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(score_thresh=0.5, weights_backbone=True, num_classes=num_classes).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr = lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_trainval, voc_test, ade_train, ade_val, ade_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "round = [\"voc\",'ade']\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_one_epoch(model, optimizer, ade_train, device, epoch=epoch, run_dataset='ade', \n",
    "                    print_freq=10, scaler=None)\n",
    "    \n",
    "    train_one_epoch(model, optimizer, voc_trainval, device, epoch=epoch, run_dataset='voc', \n",
    "                 print_freq=10, scaler=None)\n",
    "    \n",
    "    \n",
    "    lr_scheduler.step()\n",
    "        # evaluate on the test dataset3\n",
    "    evaluate(model, voc_test, device=device)\n",
    "    evaluate(model, ade_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
